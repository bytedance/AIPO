# @package _global_

defaults:
  - /experiments/stages/base
  - /runner: SelfInstructStage
  - override /hydra/launcher: process

hydra:
  job_logging:
    handlers:
      file:
        filename: "${hydra.runtime.output_dir}/self_instruct_shard_${runner.generate.shard_id}.log"

runner:
  resume: True
  n_clusters: 256
  mini_group_size: 128 # increase it if `n_instructions_to_generate_per_cluster` is large
  model:
    model_init_kwargs:
      attn_implementation: "flash_attention_2"
      torch_dtype: "bfloat16"
  max_n_generate_instructions: 2
  additional_filtering: True
  generate:
    temperature: 0.5
    top_p: 0.9
    max_new_tokens: 1024
    tensor_parallel: 1
    shard_id: "${dist.node_rank:}"
    num_shards: "${dist.nnodes:}"